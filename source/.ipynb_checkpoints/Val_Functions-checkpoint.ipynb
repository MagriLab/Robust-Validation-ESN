{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Objective Functions to minimize with Bayesian Optimization\n",
    "\n",
    "# the isnan part is necessary because in the model-informed architecture when the ESN strays\n",
    "# away from the attractor the trajectory may become unbounded\n",
    "\n",
    "def SSV(x):\n",
    "    # Single Shot Validation\n",
    "    \n",
    "    global rho, sigma_in\n",
    "    rho      = x[0]\n",
    "    sigma_in = x[1]\n",
    "        \n",
    "    #Train\n",
    "    Xa_train, Wout = train(U_washout, U_t, Y_t, tikh)[:2]\n",
    "\n",
    "    #Validate\n",
    "    Yh_val = closed_loop(N_val, Xa_train[-1], Wout)[0][1:]\n",
    "    Mean   = np.log10(np.mean((Yh_val-Y_v)**2))\n",
    "    \n",
    "    # prevent from diverging to infinity: put MSE equal to 10^10\n",
    "    if np.isnan(Mean):\n",
    "        Mean = 10\n",
    "        \n",
    "    return Mean\n",
    "\n",
    "def WFV(x):\n",
    "    # Walk-Forward Validation\n",
    "    \n",
    "    global rho, sigma_in\n",
    "    rho      = x[0]\n",
    "    sigma_in = x[1]\n",
    "        \n",
    "    #Train using tv: training+val\n",
    "    Xa_train, Wout, LHS0, RHS0 = train(U_washout, U_tv, Y_tv, tikh)\n",
    "    \n",
    "    N_fo = 2   # number of folds\n",
    "    Mean = 0 \n",
    "    N_in = 5*N_lyap   # interval before the first fold (training size for the WFV)\n",
    "    N_fw = N_val # how many steps forward the validation interval is shifted \n",
    "      \n",
    "    #Different Folds in the validation set\n",
    "    for i in range(N_fo):\n",
    "        \n",
    "        p      = N_in + i*N_fw\n",
    "        Y_val  = U[N_washout + p : N_washout + p + N_val]\n",
    "        \n",
    "        #Train: remove the part before and after the training set you are using\n",
    "        Xa1    = Xa_train[1:i*N_fw+1]\n",
    "        Y_t1   = Y_tv[:i*N_fw]  #Xa_train and Y_tv indices are shifted by one\n",
    "        Xa2    = Xa_train[p+1:]\n",
    "        Y_t2   = Y_tv[p:]   \n",
    "\n",
    "        LHS   = LHS0 - np.dot(Xa1.T, Xa1)  - np.dot(Xa2.T, Xa2)\n",
    "        RHS   = RHS0 - np.dot(Xa1.T, Y_t1) - np.dot(Xa2.T, Y_t2) \n",
    "    \n",
    "        Wout  = np.linalg.solve(LHS, RHS)\n",
    "         \n",
    "        #Validate\n",
    "        Yh_val = closed_loop(N_val-1, Xa_train[p], Wout)[0]\n",
    "        Mean  += np.log10(np.mean((Yh_val-Y_val)**2))\n",
    "        \n",
    "    # prevent from diverging to infinity: put MSE equal to 10^10\n",
    "    if np.isnan(Mean):\n",
    "        Mean = 10*N_fo\n",
    "            \n",
    "    return Mean/N_fo\n",
    "\n",
    "def WFC(x):\n",
    "    # chaotic Walk-Forward Validation\n",
    "    \n",
    "    global rho, sigma_in\n",
    "    rho      = x[0]\n",
    "    sigma_in = x[1]\n",
    "        \n",
    "    #Train using tv: training+val\n",
    "    Xa_train, Wout, LHS0, RHS0 = train(U_washout, U_tv, Y_tv, tikh)\n",
    "    \n",
    "    N_fo = 3   # number of folds\n",
    "    Mean = 0 \n",
    "    N_in = 6*N_lyap   # interval before the first fold (training size for the WFV)\n",
    "    N_fw = N_lyap # how many steps forward the validation interval is shifted \n",
    "      \n",
    "    #Different Folds in the validation set\n",
    "    for i in range(N_fo):\n",
    "        \n",
    "        p      = N_in + i*N_fw\n",
    "        Y_val  = U[N_washout + p : N_washout + p + N_val]\n",
    "        \n",
    "        #Train: remove the part before and after the training set you are using\n",
    "        Xa1    = Xa_train[1:i*N_fw+1]\n",
    "        Y_t1   = Y_tv[:i*N_fw]  #Xa_train and Y_tv indices are shifted by one\n",
    "        Xa2    = Xa_train[p+1:]\n",
    "        Y_t2   = Y_tv[p:]   \n",
    "\n",
    "        LHS   = LHS0 - np.dot(Xa1.T, Xa1)  - np.dot(Xa2.T, Xa2)\n",
    "        RHS   = RHS0 - np.dot(Xa1.T, Y_t1) - np.dot(Xa2.T, Y_t2) \n",
    "    \n",
    "        Wout  = np.linalg.solve(LHS, RHS)\n",
    "         \n",
    "        #Validate\n",
    "        Yh_val = closed_loop(N_val-1, Xa_train[p], Wout)[0]\n",
    "        Mean  += np.log10(np.mean((Yh_val-Y_val)**2))\n",
    "    \n",
    "    # prevent from diverging to infinity: put MSE equal to 10^10\n",
    "    if np.isnan(Mean):\n",
    "        Mean = 10*N_fo\n",
    "            \n",
    "    return Mean/N_fo\n",
    "\n",
    "\n",
    "def KFV(x):\n",
    "    # K-Fold Validation\n",
    "    \n",
    "    global rho, sigma_in\n",
    "    rho      = x[0]\n",
    "    sigma_in = x[1]\n",
    "        \n",
    "    #Train using tv: training+val\n",
    "    Xa_train, Wout, LHS0, RHS0 = train(U_washout, U_tv, Y_tv, tikh)\n",
    "    \n",
    "    N_fo = 3     # number of folds\n",
    "    Mean = 0 \n",
    "    N_in = 2*N_lyap   # interval before the first fold\n",
    "    N_fw = N_val # how many steps forward the validation interval is shifted \n",
    "      \n",
    "    #Different Folds in the validation set\n",
    "    for i in range(N_fo):\n",
    "        \n",
    "        p      = N_in + i*N_fw\n",
    "        Y_val  = U[N_washout + p : N_washout + p + N_val]\n",
    "        \n",
    "        #Train: remove the validation interval\n",
    "        Xa1    = Xa_train[p+1:p+N_val+1]\n",
    "        Y_t1   = Y_tv[p:p+N_val] #Xa_train and Y_tv indices are shifted by one\n",
    "\n",
    "        # faster this way\n",
    "        LHS   = LHS0 - np.dot(Xa1.T, Xa1)\n",
    "        RHS   = RHS0 - np.dot(Xa1.T, Y_t1)\n",
    "    \n",
    "        Wout  = np.linalg.solve(LHS, RHS)\n",
    "\n",
    "        #Validate\n",
    "        Yh_val = closed_loop(N_val-1, Xa_train[p], Wout)[0]\n",
    "        Mean  += np.log10(np.mean((Yh_val-Y_val)**2))\n",
    "        \n",
    "    # prevent from diverging to infinity: put MSE equal to 10^10\n",
    "    if np.isnan(Mean):\n",
    "        Mean = 10*N_fo\n",
    "            \n",
    "    return Mean/N_fo\n",
    "\n",
    "\n",
    "def KFC(x):\n",
    "    # chaotic K-Fold Validation\n",
    "    \n",
    "    global rho, sigma_in\n",
    "    rho      = x[0]\n",
    "    sigma_in = x[1]\n",
    "        \n",
    "    #Train using tv: training+val\n",
    "    Xa_train, Wout, LHS0, RHS0 = train(U_washout, U_tv, Y_tv, tikh)\n",
    "    \n",
    "    N_fo = 9     # number of folds\n",
    "    Mean = 0 \n",
    "    N_in = 0   # interval before the first fold\n",
    "    N_fw = N_lyap # how many steps forward the validation interval is shifted \n",
    "      \n",
    "    #Different Folds in the validation set\n",
    "    for i in range(N_fo):\n",
    "        \n",
    "        p      = N_in + i*N_fw\n",
    "        Y_val  = U[N_washout + p : N_washout + p + N_val]\n",
    "        \n",
    "        #Train: remove the validation interval\n",
    "        Xa1    = Xa_train[p+1:p+N_val+1]\n",
    "        Y_t1   = Y_tv[p:p+N_val] #Xa_train and Y_tv indices are shifted by one\n",
    "\n",
    "        LHS   = LHS0 - np.dot(Xa1.T, Xa1)\n",
    "        RHS   = RHS0 - np.dot(Xa1.T, Y_t1)\n",
    "    \n",
    "        Wout  = np.linalg.solve(LHS, RHS)\n",
    "\n",
    "        #Validate\n",
    "        Yh_val = closed_loop(N_val-1, Xa_train[p], Wout)[0]\n",
    "        Mean  += np.log10(np.mean((Yh_val-Y_val)**2))\n",
    "        \n",
    "    # prevent from diverging to infinity: put MSE equal to 10^10\n",
    "    if np.isnan(Mean):\n",
    "        Mean = 10*N_fo\n",
    "            \n",
    "    return Mean/N_fo\n",
    "\n",
    "def RV(x):\n",
    "    # Recycle Validation\n",
    "    \n",
    "    global rho, sigma_in    #they are used in Functions.ipynb\n",
    "    rho      = x[0]\n",
    "    sigma_in = x[1] \n",
    "        \n",
    "    #Train using tv: training+val\n",
    "    Xa_train, Wout = train(U_washout, U_tv, Y_tv, tikh)[:2]\n",
    "    \n",
    "    N_fo = 3     # number of folds\n",
    "    Mean = 0 \n",
    "    N_in = 2*N_lyap   # interval before the first fold\n",
    "    N_fw = N_val # how many steps forward the validation interval is shifted \n",
    "      \n",
    "    #Different Folds in the validation set\n",
    "    for i in range(N_fo):\n",
    "        \n",
    "        p      = N_in + i*N_fw\n",
    "        \n",
    "        # validation data for each fold\n",
    "        Y_val  = U[N_washout+p:N_washout+p+N_val]\n",
    "        #Closed-loop evolution in validation interval\n",
    "        Yh_val = closed_loop(N_val-1, Xa_train[p], Wout)[0]\n",
    "        # log10 of Mean Squared Error\n",
    "        Mean  += np.log10(np.mean((Yh_val-Y_val)**2))\n",
    "           \n",
    "    # prevent from diverging to infinity: put MSE equal to 10^10\n",
    "    if np.isnan(Mean):\n",
    "        Mean = 10*N_fo\n",
    "        \n",
    "    return Mean/N_fo\n",
    "\n",
    "def RVC(x):\n",
    "    # Chaotic Recycle Validation\n",
    "    \n",
    "    global rho, sigma_in    #they are used in Functions.ipynb\n",
    "    rho      = x[0]\n",
    "    sigma_in = x[1] \n",
    "        \n",
    "    #Train\n",
    "    Xa_train, Wout = train(U_washout, U_tv, Y_tv, tikh)[:2]\n",
    "    \n",
    "    N_fo = 9      # number of folds\n",
    "    Mean = 0 \n",
    "    N_in = 0      # interval before the first fold\n",
    "    N_fw = N_lyap # how many steps forward the validation interval is shifted \n",
    "      \n",
    "    #Different Folds in the validation set\n",
    "    for i in range(N_fo):\n",
    "        \n",
    "        p     = N_in + i*N_fw\n",
    "        \n",
    "        # validation data for each fold\n",
    "        Y_val  = U[N_washout+p:N_washout+p+N_val]\n",
    "        #Closed-loop evolution in validation interval\n",
    "        Yh_val = closed_loop(N_val-1, Xa_train[p], Wout)[0]\n",
    "        # log10 of Mean Squared Error\n",
    "        Mean  += np.log10(np.mean((Yh_val-Y_val)**2))\n",
    "        \n",
    "    # prevent from diverging to infinity: put MSE equal to 10^10\n",
    "    if np.isnan(Mean):\n",
    "        Mean = 10*N_fo\n",
    "        \n",
    "    return Mean/N_fo\n",
    "\n",
    "def Test(x):\n",
    "    # MSE in the test set\n",
    "    \n",
    "    global rho, sigma_in\n",
    "    rho      = x[0]\n",
    "    sigma_in = x[1]\n",
    "        \n",
    "    #Train\n",
    "    Wout = train(U_washout, U_tv, Y_tv, tikh)[1]\n",
    "    \n",
    "    N_test = 100       # number of test intervals\n",
    "    Mean   = 0 \n",
    "    N_in   = 24*N_lyap   # interval before the test interval (24 to have the same of the Long dataset)\n",
    "    N_fw   = N_val       # how many steps forward the test interval is shifted \n",
    "    \n",
    "    for i in range(N_test):\n",
    "        \n",
    "        p         = N_in + i*N_fw\n",
    "        \n",
    "        # data for washout and target in each interval\n",
    "        U_wash   = U[p - N_washout : p]\n",
    "        Y_test   = U[p:p+N_val] \n",
    "        \n",
    "        #washout for each interval\n",
    "        xa1      = open_loop(U_wash, np.zeros(N_units))[-1]\n",
    "        \n",
    "        # Mean Square Error\n",
    "        Yh_test  = closed_loop(N_val-1, xa1, Wout)[0]\n",
    "        Mean1    = np.log10(np.mean((Yh_test - Y_test)**2))\n",
    "        if np.isnan(Mean1):\n",
    "            Mean1 = 10\n",
    "        Mean    += Mean1\n",
    "        \n",
    "    return Mean/N_test\n",
    "\n",
    "\n",
    "def Test1(x):\n",
    "    # MSE in the test set interval from 12 to 15 LT\n",
    "    \n",
    "    global rho, sigma_in\n",
    "    rho      = x[0]\n",
    "    sigma_in = x[1]\n",
    "        \n",
    "    #Train\n",
    "    Wout = train(U_washout, U_tv, Y_tv, tikh)[1]\n",
    "    \n",
    "    N_test = 1       # number of test intervals\n",
    "    Mean   = 0 \n",
    "    N_in   = 12*N_lyap   # interval before the test interval (24 to have the same of the Long dataset)\n",
    "    N_fw   = N_val       # how many steps forward the test interval is shifted \n",
    "    \n",
    "    for i in range(N_test):\n",
    "        \n",
    "        p         = N_in + i*N_fw\n",
    "        \n",
    "        # data for washout and target in each interval\n",
    "        U_wash   = U[p - N_washout : p]\n",
    "        Y_test   = U[p:p+N_val] \n",
    "        \n",
    "        #washout for each interval\n",
    "        xa1      = open_loop(U_wash, np.zeros(N_units))[-1]\n",
    "        \n",
    "        # Mean Square Error\n",
    "        Yh_test  = closed_loop(N_val-1, xa1, Wout)[0]\n",
    "        Mean1    = np.log10(np.mean((Yh_test - Y_test)**2))\n",
    "        if np.isnan(Mean1):\n",
    "            Mean1 = 10\n",
    "        Mean    += Mean1\n",
    "        \n",
    "    return Mean/N_test\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
